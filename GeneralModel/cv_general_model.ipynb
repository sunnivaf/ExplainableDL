{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyxdf\n",
    "\n",
    "# mne imports\n",
    "import mne\n",
    "from mne import io\n",
    "from mne.datasets import sample\n",
    "\n",
    "# EEGNet-specific imports\n",
    "from EEGModels import EEGNet\n",
    "import tensorflow\n",
    "from tensorflow.keras import utils as np_utils\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# PyRiemann imports\n",
    "from pyriemann.estimation import XdawnCovariances\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "\n",
    "#Sklearn imports\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import compute_class_weight\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# tools for plotting confusion matrices\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pyriemann.utils.viz import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 1233\n",
    "\n",
    "X_train_01 = np.loadtxt(\"Full_data_X_P01_5.csv\")\n",
    "X_train_02 = np.loadtxt(\"Full_data_X_P02_5.csv\")\n",
    "X_train_03 = np.loadtxt(\"Full_data_X_P03_5.csv\")\n",
    "X_train_04 = np.loadtxt(\"Full_data_X_P04_5.csv\")\n",
    "X_train_05 = np.loadtxt(\"Full_data_X_P05_5.csv\")\n",
    "X_train_06 = np.loadtxt(\"Full_data_X_P06_5.csv\")\n",
    "X_train_07 = np.loadtxt(\"Full_data_X_P07_5.csv\")\n",
    "X_train_08 = np.loadtxt(\"Full_data_X_P08_5.csv\")\n",
    "X_train_09 = np.loadtxt(\"Full_data_X_P09_5.csv\")\n",
    "X_train_10 = np.loadtxt(\"Full_data_X_P10_5.csv\")\n",
    "X_train_11 = np.loadtxt(\"Full_data_X_P11_5.csv\")\n",
    "X_train_12 = np.loadtxt(\"Full_data_X_P12_5.csv\")\n",
    "X_train_13 = np.loadtxt(\"Full_data_X_P13_5.csv\")\n",
    "X_train_14 = np.loadtxt(\"Full_data_X_P14_5.csv\")\n",
    "X_train_15 = np.loadtxt(\"Full_data_X_P15_5.csv\")\n",
    "X_train_16 = np.loadtxt(\"Full_data_X_P16_5.csv\")\n",
    "X_train_17 = np.loadtxt(\"Full_data_X_P17_5.csv\")\n",
    "X_train_18 = np.loadtxt(\"Full_data_X_P18_5.csv\")\n",
    "X_train_19 = np.loadtxt(\"Full_data_X_P19_5.csv\")\n",
    "X_train_20 = np.loadtxt(\"Full_data_X_P20_5.csv\")\n",
    "\n",
    "\n",
    "X_train_01 = X_train_01.reshape(\n",
    "     X_train_01.shape[0], X_train_01.shape[1] // samples, samples)\n",
    "\n",
    "X_train_02 = X_train_02.reshape(\n",
    "     X_train_02.shape[0], X_train_02.shape[1] // samples, samples)\n",
    "\n",
    "X_train_03 = X_train_03.reshape(\n",
    "     X_train_03.shape[0], X_train_03.shape[1] // samples, samples)\n",
    "\n",
    "X_train_04 = X_train_04.reshape(\n",
    "     X_train_04.shape[0], X_train_04.shape[1] // samples, samples)\n",
    "\n",
    "X_train_05 = X_train_05.reshape(\n",
    "     X_train_05.shape[0], X_train_05.shape[1] // samples, samples)\n",
    "\n",
    "X_train_06 = X_train_06.reshape(\n",
    "     X_train_06.shape[0], X_train_06.shape[1] // samples, samples)\n",
    "\n",
    "X_train_07 = X_train_07.reshape(\n",
    "     X_train_07.shape[0], X_train_07.shape[1] // samples, samples)\n",
    "\n",
    "X_train_08 = X_train_08.reshape(\n",
    "     X_train_08.shape[0], X_train_08.shape[1] // samples, samples)\n",
    "\n",
    "X_train_09 = X_train_09.reshape(\n",
    "     X_train_09.shape[0], X_train_09.shape[1] // samples, samples)\n",
    "\n",
    "X_train_10 = X_train_10.reshape(\n",
    "     X_train_10.shape[0], X_train_10.shape[1] // samples, samples)\n",
    "\n",
    "X_train_11 = X_train_11.reshape(\n",
    "     X_train_11.shape[0], X_train_11.shape[1] // samples, samples)\n",
    "\n",
    "X_train_12 = X_train_12.reshape(\n",
    "     X_train_12.shape[0], X_train_12.shape[1] // samples, samples)\n",
    "\n",
    "X_train_13 = X_train_13.reshape(\n",
    "     X_train_13.shape[0], X_train_13.shape[1] // samples, samples)\n",
    "\n",
    "X_train_14 = X_train_14.reshape(\n",
    "     X_train_14.shape[0], X_train_14.shape[1] // samples, samples)\n",
    "\n",
    "X_train_15 = X_train_15.reshape(\n",
    "     X_train_15.shape[0], X_train_15.shape[1] // samples, samples)\n",
    "\n",
    "X_train_16 = X_train_16.reshape(\n",
    "     X_train_16.shape[0], X_train_16.shape[1] // samples, samples)\n",
    "\n",
    "X_train_17 = X_train_17.reshape(\n",
    "     X_train_17.shape[0], X_train_17.shape[1] // samples, samples)\n",
    "\n",
    "X_train_18 = X_train_18.reshape(\n",
    "     X_train_18.shape[0], X_train_18.shape[1] // samples, samples)\n",
    "\n",
    "X_train_19 = X_train_19.reshape(\n",
    "     X_train_19.shape[0], X_train_19.shape[1] // samples, samples)\n",
    "\n",
    "X_train_20 = X_train_20.reshape(\n",
    "     X_train_20.shape[0], X_train_20.shape[1] // samples, samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_training = ([X_train_20, X_train_19, X_train_18, X_train_17, X_train_16, X_train_15, X_train_14, X_train_13, X_train_12, X_train_11, X_train_10, X_train_09, X_train_08, \n",
    "                        X_train_07, X_train_06, X_train_05, X_train_04, X_train_03, X_train_02])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_01 = np.loadtxt(\"Full_data_Y_P01_5.csv\")\n",
    "Y_train_02 = np.loadtxt(\"Full_data_Y_P02_5.csv\")\n",
    "Y_train_03 = np.loadtxt(\"Full_data_Y_P03_5.csv\")\n",
    "Y_train_04 = np.loadtxt(\"Full_data_Y_P04_5.csv\")\n",
    "Y_train_05 = np.loadtxt(\"Full_data_Y_P05_5.csv\")\n",
    "Y_train_06 = np.loadtxt(\"Full_data_Y_P06_5.csv\")\n",
    "Y_train_07 = np.loadtxt(\"Full_data_Y_P07_5.csv\")\n",
    "Y_train_08 = np.loadtxt(\"Full_data_Y_P08_5.csv\")\n",
    "Y_train_09 = np.loadtxt(\"Full_data_Y_P09_5.csv\")\n",
    "Y_train_10 = np.loadtxt(\"Full_data_Y_P10_5.csv\")\n",
    "Y_train_11 = np.loadtxt(\"Full_data_Y_P11_5.csv\")\n",
    "Y_train_12 = np.loadtxt(\"Full_data_Y_P12_5.csv\")\n",
    "Y_train_13 = np.loadtxt(\"Full_data_Y_P13_5.csv\")\n",
    "Y_train_14 = np.loadtxt(\"Full_data_Y_P14_5.csv\")\n",
    "Y_train_15 = np.loadtxt(\"Full_data_Y_P15_5.csv\")\n",
    "Y_train_16 = np.loadtxt(\"Full_data_Y_P16_5.csv\")\n",
    "Y_train_17 = np.loadtxt(\"Full_data_Y_P17_5.csv\")\n",
    "Y_train_18 = np.loadtxt(\"Full_data_Y_P18_5.csv\")\n",
    "Y_train_19 = np.loadtxt(\"Full_data_Y_P19_5.csv\")\n",
    "Y_train_20 = np.loadtxt(\"Full_data_Y_P20_5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_training = ([Y_train_20, Y_train_19, Y_train_18, Y_train_17, Y_train_16, Y_train_15, Y_train_14, Y_train_13, Y_train_12, Y_train_11, Y_train_10, Y_train_09, Y_train_08, \n",
    "                        Y_train_07, Y_train_06, Y_train_05, Y_train_04, Y_train_03, Y_train_02])\n",
    "\n",
    "print(len(Y_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_testing = np.vstack([X_train_01])\n",
    "Y_testing = np.hstack([Y_train_01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels, chans, samples = 1, 16, 1233\n",
    "\n",
    "model = EEGNet(nb_classes = 1, Chans = chans, Samples = samples, \n",
    "               dropoutRate = 0.25, kernLength = 125, F1 = 8, D = 2, F2 = 16, \n",
    "               dropoutType = 'Dropout')\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001),\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 3\n",
    "kf = KFold(n_splits=num_folds, shuffle=False)\n",
    "accuracy = np.zeros(num_folds)\n",
    "F1_score = np.zeros(num_folds)\n",
    "precision = np.zeros(num_folds)\n",
    "recall = np.zeros(num_folds)\n",
    "specificity = np.zeros(num_folds)\n",
    "i=0\n",
    "\n",
    "for train_index, test_index in kf.split(X_training, Y_training):\n",
    "\n",
    "    if train_index.shape[0]==13:\n",
    "        X_training_val = np.vstack([X_training[train_index[0]], X_training[train_index[1]], X_training[train_index[2]], X_training[train_index[3]], X_training[train_index[4]], X_training[train_index[5]], X_training[train_index[6]],\n",
    "                                X_training[train_index[7]], X_training[train_index[8]], X_training[train_index[9]], X_training[train_index[10]], X_training[train_index[11]], X_training[train_index[12]]])\n",
    "        Y_training_val = np.hstack([Y_training[train_index[0]], Y_training[train_index[1]], Y_training[train_index[2]], Y_training[train_index[3]], Y_training[train_index[4]], Y_training[train_index[5]], Y_training[train_index[6]],\n",
    "                                Y_training[train_index[7]], Y_training[train_index[8]], Y_training[train_index[9]], Y_training[train_index[10]], Y_training[train_index[11]], Y_training[train_index[12]]])\n",
    "\n",
    "        X_test_val = np.vstack([X_training[test_index[0]], X_training[test_index[1]], X_training[test_index[2]], X_training[test_index[3]], X_training[test_index[4]], X_training[test_index[5]]])\n",
    "        Y_test_val = np.hstack([Y_training[test_index[0]], Y_training[test_index[1]], Y_training[test_index[2]], Y_training[test_index[3]], Y_training[test_index[4]], Y_training[test_index[5]]])\n",
    "\n",
    "    else:\n",
    "        X_training_val = np.vstack([X_training[train_index[0]], X_training[train_index[1]], X_training[train_index[2]], X_training[train_index[3]], X_training[train_index[4]], X_training[train_index[5]], X_training[train_index[6]],\n",
    "                                X_training[train_index[7]], X_training[train_index[8]], X_training[train_index[9]], X_training[train_index[10]], X_training[train_index[11]]])\n",
    "        Y_training_val = np.hstack([Y_training[train_index[0]], Y_training[train_index[1]], Y_training[train_index[2]], Y_training[train_index[3]], Y_training[train_index[4]], Y_training[train_index[5]], Y_training[train_index[6]],\n",
    "                                Y_training[train_index[7]], Y_training[train_index[8]], Y_training[train_index[9]], Y_training[train_index[10]], Y_training[train_index[11]]])\n",
    "\n",
    "        X_test_val = np.vstack([X_training[test_index[0]], X_training[test_index[1]], X_training[test_index[2]], X_training[test_index[3]], X_training[test_index[4]], X_training[test_index[5]],  X_training[test_index[6]]])\n",
    "        Y_test_val = np.hstack([Y_training[test_index[0]], Y_training[test_index[1]], Y_training[test_index[2]], Y_training[test_index[3]], Y_training[test_index[4]], Y_training[test_index[5]],  Y_training[test_index[6]]])\n",
    "\n",
    "    #Standardising the data\n",
    "    scaler = StandardScaler()\n",
    "    X_training_val = scaler.fit_transform(X_training_val.reshape(X_training_val.shape[0], -1)).reshape(X_training_val.shape)\n",
    "    X_test_val = scaler.fit_transform(X_test_val.reshape(X_test_val.shape[0], -1)).reshape(X_test_val.shape)\n",
    "\n",
    "    #Shuffling the data\n",
    "    randomize_train = np.arange(len(X_training_val))\n",
    "    np.random.shuffle(randomize_train)\n",
    "    X_training_val = X_training_val[randomize_train]\n",
    "    y_train = Y_training_val[randomize_train]\n",
    "\n",
    "    randomize_val = np.arange(len(X_test_val))\n",
    "    np.random.shuffle(randomize_val)\n",
    "    X_test_val = X_test_val[randomize_val]\n",
    "    y_val = Y_test_val[randomize_val]\n",
    "\n",
    "    # convert data to (trials, channels, samples, kernels) format. Data \n",
    "    # contains 16 channels and 7400 time-points. Set the number of kernels to 1.\n",
    "\n",
    "    X_training_val  = X_training_val.reshape(X_training_val.shape[0], chans, samples, kernels)\n",
    "    X_test_val   = X_test_val.reshape(X_test_val.shape[0], chans, samples, kernels)\n",
    "\n",
    "    #Train the model\n",
    "    fittedModel = model.fit(X_training_val, y_train, batch_size = 1024, epochs = 20)\n",
    "    \n",
    "    score = model.evaluate(X_test_val, y_val)\n",
    "\n",
    "    pred_val = model.predict(X_test_val)\n",
    "    #print(\"Pred val: \", pred_val)\n",
    "    pred_val = np.where(pred_val > 0.5, 1, 0)\n",
    "\n",
    "    #Plotting confusion matrix\n",
    "    labels = [\"Non-Alcohol\", \"Alcohol\"]\n",
    "    cm = confusion_matrix(y_val, pred_val)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.show()\n",
    "    \n",
    "    tn = cm[0][0] #true negatives\n",
    "    fn = cm[0][1] #false positives\n",
    "\n",
    "    print(f\"Accuracy for the fold no. {i+1} on the test set: {score[1]}\")\n",
    "    accuracy[i] = score[1]\n",
    "    F1_score[i] = f1_score(y_true=y_val, y_pred=pred_val)\n",
    "    precision[i] = precision_score(y_true=y_val, y_pred=pred_val)\n",
    "    recall[i] = recall_score(y_true=y_val, y_pred=pred_val)\n",
    "    specificity[i] = tn/(tn+fn)\n",
    "    \n",
    "\n",
    "    pred_val = model.predict(X_test_val)\n",
    "    pred_val = np.where(pred_val > 0.5, 1, 0)\n",
    "\n",
    "    #How many occurances appear in the train set\n",
    "    unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
    "    unique_test, counts_test = np.unique(y_val, return_counts=True)\n",
    "    # print(np.asarray((unique_train, counts_train)).T)\n",
    "    # print(np.asarray((unique_test, counts_test)).T)\n",
    "\n",
    "    i+=1\n",
    "\n",
    "\n",
    "standard_deviation = np.std(accuracy)\n",
    "\n",
    "print(\"The accuracy of the model with cross validation is\", accuracy.mean())\n",
    "print(\"The precision score of the model with cross validation is\", precision.mean())\n",
    "print(\"The recall score of the model with cross validation is\", recall.mean())\n",
    "print(\"The F1 score of the model with cross validation is\", F1_score.mean())\n",
    "print(\"The specificity score of the model with cross validation is\", specificity.mean())\n",
    "print(\"The standard deviation of the accuracy of the model with cross validation is\", standard_deviation)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
